- set of libraries
- core components of map reduce:
	- Job: unit of MapReduce work/instance
		- we will create a job class and then we will also create an instance of the class in our code
	- map task: runs on each node
	- reducer task: runs on some nodes
	- source data - HDFS or other location
		- type and location
		- by default it will be found in the HDFS
- map reduce works with many daemons and services
- each one of those daemons is a separate JVM and it's isolated
	- in MR1, the daemons: Job tracker(one, it is a controller and scheduler), task trackers: one per cluster(monitors tasks)
- we will be writitng job configurations
	- specify i/o locations for job instances
	- job client submits these configs for execution
- sort in the below diagram is default
	- copy and merge can be overrided
![[Pasted image 20230710175702.png]]

- mapreduce coding patterns:
	- standard - usually written in Java
	- Hadoop Streaming - Java Base
	- other langs
	- hadoop pipes - C++
- hdfs data is immutable
	- new loc should be configured for each run or the job will fail saying that there is data already present
- for other FS, we can configure to overwrite or append, overwrite is by default
- MR jobs can be long-running
	- open-source distros of hadoop contains a couple of local websites, which give us very simple job status application
	- cloudera has hue
	- log files also can be read to understand the status of the job
		- we can use monitoring tools to read our log files and display the anamolies
- mr logs are verbose
	- might be too much for production