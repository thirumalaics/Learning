- setting up Hadoop cluster we have number of options
- using VMs
	- either vendor provided(cloudera etc) or create our own image
- using docker
	- build docker image from Dockerfile
	- base image is key

## MapReduce
- programming paradigm
- designed to solve one problem
	- indexing the web
- two parts 
	- map and reduce
- write Map() on data
	- some function on data
	- run on each node where the data lives
	- if a node containing a block of data goes down, map job retried in another node containing that piece of data
	- by this way, the compute is brought to the data
	- set of key, value pairs are output on each node
- executes the Reduce() function on data
	- executes only on some nodes of the cluster
	- reducer aggregates sets of key,value pairs on some nodes
	- output is a single combined list
![[Pasted image 20230707175810.png]]
- data that comes out of the mapper gets agged(shuffle) in the reducer where the reducer performs our logic
	- shuffling can be automatic and also can be coded
- v1 for specific problems
	- distributed, scalable, cheap
- coding process:
	- create a class
		- create a static Map, Reduce fn
		- main fn
			- contains implementation of a job runner
			- the job calls the Map and reduce classes
![[Pasted image 20230707180405.png]]
- when we run a map fn, the fn runs in a JVM on each of those nodes
![[Pasted image 20230707180936.png]]