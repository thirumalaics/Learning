- beam APIs enable users to build their own sources
- the root node of the pipeline
- BoundedSource - API
	- read from a finite data source in a batch pipeline
		- FileIO, RedisIO
	- fast, more parallelisms
	- APIs: BoundedSource, BoundedReader

- UnboundedSource - API
	- read from an infinite data source in a streaming pipeline
		- KafkaIO
	- advance wm correctly, drain, checkpoint
	- APIs: UnboundedSource, UnboundedReader, CheckpointMark

## What is SDF?
- it is a DoFn
	- same syntax as DoFn: @StartBundle, @FinishBundle, @ProcessElement
		- but we cannot use user states and timers inside one SDF
	- responsible for processing element and restriction pairs
		- Restriction: represents a subset of work that would have been necessary to been done when processing the element
		- Kafka example: element -> Kafka TopicPartition, restriction -> an OffsetRange\[0,Inf)
			- Kafka's topics are divided into several partitions
			- Topic is a logical concept
				- a partition is the smallest storage unit that holds a subset of records owned by a topic
				- each partition is a single log file where records/messages are written to it in an append-only fashion
			- the records in the partitions are each assigned a sequential identifier called the offset
				- unique for a record within the partition
				- inc and immutable number, maintained by Kafka
				- useful for consumers when reading records from a partition
				- unlike other pub/sub imp, Kafka doesnt push messages to consumers
					- they have to pull messages off kafka topic partitions
				- a consumer connects to a partition in a broker, reads the messages in the order in which they were written
				- offset of a message works as a consumer side cursor at this point
				- consumer keeps track of offsets that are already read
					- responsibility of consumer
- the ability to split inside one element gives the name SDF
- element is usually the metadata or source descriptor, describing what we have to read
- restriction usually describe the amount of data
- to connect to an unsupported data store in the BeamSDK, we need to create a custom I/O connector
- until recently, we had two options
	- Create a mini-pipeline made of the basic ParDo and GroupByKey transforms(bounded sources only and certain scenarios)
	- Use the SourcesAPI
- Connectors as Mini-pipelines:
- for bds, where data can be read in parallel, a mini-pipeline can be created consisting of two steps:
	- split incoming data into parts to be read in parallel
	- read from each of those parts
- two pardos used, one to split the data and the other to read from the splitted data
- the pardos were separated by GBK in b/w
![[Pasted image 20230618105332.png]]
![[Pasted image 20230618105453.png]]