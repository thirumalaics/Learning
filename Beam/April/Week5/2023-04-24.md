- to use beam, we need to first create a driver program using the classes in one of the Beam SDKs
- our driver program defines our pipeline
	- including all the i/ps, transforms and o/ps
	- also sets execution options for our pp
		- which can be passed as CL options
		- includes pipeline runner
			- which in turn decides what back-end our pipeline will run on
- when we create a Pipeline object, we must also specify the execution options that tell the pipeline where and how to run
- all beam driver programs must create a pipeline
- Beam comes with multiple IOs library PTransforms
	- read or write data from/to various ext storage sys
- a typical beam driver program:
	1. Create a Pipeline object and set the pipeline execution options
		- set execution options, including the pipeline runner
	2. Create an initial PCollection for pipeline data
		- either use IOs to read data or using Create transform to build a Pcollection from in-memory data
	3. Apply PTransforms to each Pcollection
		- a transform creates a new o/p PCollection without modifying the i/p collection
	4. use IOs to write the final transformed Pcollection(s) to external sources
	5. Run the Pipeline using the designated Runner
- when we run our Beam driver program, the Pipeline Runner constructs a WF graph of our pipeline
	- based on the Pcoll objs that we have created and transforms we have applied
```
import apache_beam as beam

with beam.Pipeline() as pipeline:
    pass  # build your pipeline here
```
- when creating pipeline obj, we can set options
	- easier
	- we can also set it programmatically later

## Setting PipelineOptions from Command-line args
- we can configure our pipeline by creating a PipelineOptions object and setting fields directly
- BeamSDK also allows us to include a command-line parser that we can use to set fields in PiplineOptions using CL args
```
from apache_beam.options.pipeline_options import PipelineOptions

beam_options = PipelineOptions()
```

- the above interprets CL args that follow the format:
	- --\<option\>=\<value\>
- this way lets us specify any of the exec options as a CL arg

## Creating custom options
- we can add our own custom options in addition to standard PipelineOptions
```
from apache_beam.options.pipeline_options import PipelineOptions
class MyOptions(PipelineOptions):
	@classmethod
	def _add_argparse_args(cls, parser):
		parser.add_argument('--input',
							default='gs://dataflow-samples/shakespeare/kinglear.txt',
					        help='The file path for the input text to process.')
		parser.add_argument('--output', required=True,
							help='The path prefix for output files.')
```
- for Python, we can also simply parse our custom options with argparse
- there is no need to create separate PipelineOptions subclass
