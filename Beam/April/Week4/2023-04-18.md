## Introduction to Data Processing with Apache Beam
## Why Apache Beam
- two questions to to ask when considering a new tech to learn and apply?
	1. What problem am I struggling with, that the new technology can help me solve?
	2. What would the costs associated with the technology be?
- selling point of Beam: portability
	- portable on several layers:
		1. Pipelines are portable b/w multiple runners(that is, a tech that executes the distributed computation described by a pipeline's author)
		2. Beam's data processing model is portable b/w various programming languages
			- i can use beam java code as a function in python
		3. Beam's data processing logic is portable b/w bounded and unbounded data

- runner portability: choice of running existing pipelines written in one of the supported programming languages(ex: Py,Java, etc..) against a data processing engine that can be chosen at runtime
	- trypical ex of a runner: Apache Flink, Apache Spark, Google Cloud Dataflow
- new runners are created, Beam not limited to the above runners
- as Beam's data processing model is portable b/w various programming languages, it has the ability to provide support for multiple SDKs, regardless of the language or tech used by the runner
	- meaning: Beam pipelines in the Go Language can be run against the Apache Flink Runner, which is written in Java

# SDK v API
- both enable us to enhance the functionality of our app with relative ease
## SDK
- s/w development kit
- aka devkit
- the sdk is a set of s/w building tools for a specific p/f
- set of tools provided by the manufacturer of usually a h/w platform, operating system(OS), or programming language
- what is included in the kit varies from manufacturer to manufacturer
- typically, a basic SDK will include a compiler, debugger, and APIs
- a good sdk will provide all components a developer might find necessary when creating new apps for that specific product and its ecosystem
- ex: MacOs X SDK, iPhone SDK