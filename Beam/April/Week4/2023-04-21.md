## PTransform
- dataprocessing operation
- aka transform
	- includes read and write
- usually applied to one or more input PCollection objects
- Transforms that read input are an exception
	- might not have an i/p PColl
- we provide transform processing logic in the form of a fn obj
	- aka user code
	- our user code is applied to each element of the i/p PCollection
		- or more than one PCollection
	- depending on the pipeline runner and backend that we choose, many diff workers across a cluster might execute instances of our user code in parallel
	- the user code that executes, generates o/p elements that are added to zero or more o/p Pcoll objs
- Beam SDK contains number of pre-written transforms
	- complex transformations are available as well
	- we can even define our own

## Aggregation
- computing one value from multiple i/p elements
- primary computational pattern for aggregation is to group all elements with a common key and window(window is optional for group) then combine each group of elements using an [associative and commutative](https://www.toppr.com/guides/maths/factorisation/associative-and-commutative-property-of-addition-and-multiplication-with-examples/#:~:text=The%20commutative%20property%20deals%20with,%2B%20(b%20%2B%20c).) operation
- when elements are grouped and emitted as a bag, the agg is known as GroupByKey
	- the associative and commutative operation is bag union
	- the op is not smaller than the ip
- often we will apply an operation such as summation: called CombineFn -> in which the o/p is significantly smaller than the i/p
	- in this case the agg is called CombinePerKey

## User-defined function
- some beam operations allow us to run user-defined code as a way to configure the transform
- a beam pipeline can contain UDFs written in a diff lang or even multiple langs in the same pipeline

## Schema
- schema for a PColletion defines elements of that PCollection as an ordered list of named fields
- each field has a name, a type, and possibly a set of user options
- the element type in a PColl has a structure that can be introspected
- by understanding the structure of a pipeilne's records, we can provide much more concise APIs for data processing

## Runner
- a runner runs a Beam pipeline on a Specific platform
- most runners are translators or adapters to massively parallel big data processing systems
	- ex: Flink, Spark, Dataflow and more
	- Flink runner translates a Beam pipeline into a Flink Job
	- direct runner runs pipeline locally so we can test, debug and validate